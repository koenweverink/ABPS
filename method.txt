Given a set of enemy units, determine the best allocation and composition of friendly units to defeat them efficiently (e.g. lowest casualties, fastest mission, highest score).

1. Define COA Representation

You need a way to encode a "COA" — a particular deployment plan. For example:
[
    ("EnemyTankGroup1", "FriendlyTank", 2),
    ("EnemyInfantryGroup1", "FriendlyInfantry", 1),
    ...
]

This will be your search space.


2. Generate COA Variants

You’ll want to try hundreds or thousands of combinations:

    Use random sampling first (Monte Carlo)

    Optionally apply genetic algorithms or Bayesian optimization later

Each trial involves:

    Picking friendly unit counts and types

    Assigning them to specific enemy groups



3. Simulation Loop

For each COA:

    Generate friendly_units based on the COA

    Initialize the Simulation(...)

    Run .run(max_steps=...)

    Save the output score and COA


4. Evaluate

Track:

    Total score

    Friendly health remaining

    Time to outpost secure

    Casualty rate

Sort and pick best.




✅ Goal: Hybrid HTN+RL Control
HTN does:

    “Secure the outpost”, “AttackEnemy”, “Move”

RL does:

    How to move, when to fire, how far to retreat, when to flank


1. Let HTN produce high-level plan

In each step, your friendly unit's update_plan() gives something like:

[("AttackEnemy", "EnemyTank")]

This becomes the context or “intent” passed to the RL policy.


2. RL policy maps (state + intent) → low-level action

Update your InfantryTankSimEnv.step(action) to:

    Let HTN assign high-level tasks (task_name, task_arg) to each unit

    RL decides how to execute that: approach, retreat, attack, etc.

